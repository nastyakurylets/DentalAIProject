{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd34e17c",
   "metadata": {},
   "source": [
    "## Система автоматичного розпізнавання патологій на знімках зубів"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b87f7",
   "metadata": {},
   "source": [
    "#### Команда\n",
    "Топоровська Вікторія, Курилець Анастасія, Чеботарьова Юлія\n",
    "\n",
    "#### Ідея\n",
    "Cтворити модель, яка може дозволити автоматизувати процес діагностики зубів, і зробити цю процедуру доступнішою.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b43bc",
   "metadata": {},
   "source": [
    "Дивлячись на можливі імплементації, для початкової демо версії ми вирішили використовувати Sequentia. Це є найпростіша модель типу CNN, з лінійною послідовністю.\n",
    "Починаємо з завантажування бібліотек. \n",
    "(IMPORT може повторюватися, бо чомуся ipynb моментами ігнорує попередню ініціалізацію)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3000b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c265a",
   "metadata": {},
   "source": [
    "Грузимо наші фото. \n",
    "\n",
    "наша структура\n",
    "\n",
    "\n",
    "--- data\n",
    "\n",
    "    --- здорові\n",
    "    --- хворі\n",
    "\n",
    "\n",
    "Використовуємо ImageDataGenerator він автоматично зчитує та попередньо обробляє дані. \n",
    "\n",
    "\n",
    "rescale=1./255 - масштабуємо яскравість пікселя з діапазону [0-225] до [0, 1]. щоб модель працювала краще і швидше. 225 - найяскравіше можливе знчаення для пікселя\n",
    "\n",
    "flow_from_directory() - щоб не підвантажувати зображення окремо з кожної папки.\n",
    "\n",
    "target_size=(150, 150) — усі зображення масштабуються до одного розміру 150×150 пікселів. Це потрібно, тому що нейронна мережа приймає вхідні дані однакових розмірів. Якщо зображення різні за розміром — навчання буде неможливе.\n",
    "\n",
    "batch_size=32 — визначає, скільки зображень обробляється за один крок. Малий batch робить навчання повільнішим, але точнішим; великий — швидшим, але менш стабільним. 32 — це збалансоване значення\n",
    "\n",
    "class_mode='binary' - задає тип класифікації, у нашій початковій спробі це хворий чи здоровий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca51b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 396 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #знову ж таки чомусь бачить з попередньої комірки через раз\n",
    "\n",
    "\n",
    "train_dir = \"data\"  \n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,              \n",
    "    target_size=(150, 150), \n",
    "    batch_size=32,          \n",
    "    class_mode='binary'     \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197885a",
   "metadata": {},
   "source": [
    "Так як у нас є просто дата сет з даними де є хворі і не хворі зуби, ми розділяємо на тренувальні і тестувальні дані. 75% ми даємо на тренування, а 25% на тестування, створюємо всі потрібні папки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8cac19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розподіл завершено\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "\n",
    "data_dir = \"data\"  \n",
    "healthy_dir = os.path.join(data_dir, \"DENTAL_HEALTHY_TEETH\") \n",
    "diseased_dir = os.path.join(data_dir, \"DENTAL_NOT_HEALTHY_TEETH\") \n",
    "\n",
    "\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "\n",
    "for folder in [train_dir, test_dir]:\n",
    "    os.makedirs(os.path.join(folder, \"healthy\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(folder, \"diseased\"), exist_ok=True)\n",
    "\n",
    "\n",
    "def split_data(source_dir, train_dest, test_dest, train_ratio=0.75):\n",
    "    files = os.listdir(source_dir)\n",
    "    random.shuffle(files)\n",
    "    n_train = int(len(files) * train_ratio)\n",
    "    train_files = files[:n_train]\n",
    "    test_files = files[n_train:]\n",
    "\n",
    "    for f in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, f), os.path.join(train_dest, f))\n",
    "    for f in test_files:\n",
    "        shutil.copy(os.path.join(source_dir, f), os.path.join(test_dest, f))\n",
    "\n",
    "\n",
    "split_data(\n",
    "    healthy_dir,\n",
    "    os.path.join(train_dir, \"healthy\"),\n",
    "    os.path.join(test_dir, \"healthy\")\n",
    ")\n",
    "\n",
    "\n",
    "split_data(\n",
    "    diseased_dir,\n",
    "    os.path.join(train_dir, \"diseased\"),\n",
    "    os.path.join(test_dir, \"diseased\")\n",
    ")\n",
    "\n",
    "print(\"Розподіл завершено\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f09cda",
   "metadata": {},
   "source": [
    "Також ми отримуємо дві папки з даними і тепер вже працюємо з нашою моделлю"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fdf3f6",
   "metadata": {},
   "source": [
    "epochs = 20 — це кількість повних проходів через увесь навчальний набір даних. Модель 20 разів побачить усі зображення з тренувального набору і щоразу оновлюватиме свої ваги.\n",
    "\n",
    "batch_size = 16 — це кількість зображень, які мережа обробляє за один крок (перед оновленням ваг)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b240ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "\n",
    "train_data_dir = 'train'  \n",
    "test_data_dir = 'test'   \n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c8dfc",
   "metadata": {},
   "source": [
    "Створення моделі"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bc1ae",
   "metadata": {},
   "source": [
    "Наша модель складається з кількох етапів обробки зображення. Перші три шари Conv2D (із 32, 32 та 64 фільтрами розміру 3×3) послідовно виділяють важливі ознаки з рентгенівських знімків зубів — від простих контурів і форм до складніших структур. Після кожного згорткового шару йде активація ReLU, що додає нелінійність і допомагає моделі виявляти складні візуальні патерни. MaxPooling2D після кожного блоку зменшує розмірність зображення, виділяючи найважливіші ознаки й знижуючи обчислювальну складність. Потім шар Flatten перетворює двовимірні ознаки в одновимірний вектор, який подається у Dense-шар із 64 нейронами для аналізу знайдених патернів. Dropout(0.5) випадково вимикає половину нейронів під час навчання, щоб запобігти перенавчанню. Нарешті, вихідний Dense(1) із активацією sigmoid видає ймовірність наявності патології — значення близьке до 1 означає патологію, а до 0 — норму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deeb0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107854b",
   "metadata": {},
   "source": [
    "Компіляція"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07a9c0",
   "metadata": {},
   "source": [
    "Компіляція це налаштування нашої моделі, тобто як саме вона буде вчитися, що саме буде оптимізувати і як оцінювати результати\n",
    "\n",
    "loss='binary_crossentropy' — порівнює передбачене значення (ймовірність патології) із правильним класом (0 або 1) і вимірює, наскільки сильно модель помиляється\n",
    "\n",
    "optimizer='rmsprop' — це алгоритм, який оновлює ваги моделі на основі градієнтів\n",
    "\n",
    "metrics=['accuracy'] — вказує, наточність (accuracy), тобто частку правильних передбачень."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d72afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64975b",
   "metadata": {},
   "source": [
    "генератори даних"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ef0ff",
   "metadata": {},
   "source": [
    "щоб наша модель не лякалася шумів, і зайвих поворотів розворотів, ми самостійно генеруємо якісь невеличкі повороти і шуми, щоб модель вчилася працювати і зними"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6867a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370 images belonging to 2 classes.\n",
      "Found 174 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafdb2b",
   "metadata": {},
   "source": [
    "Навчання моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0250070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.4944 - loss: 0.8711 - val_accuracy: 0.5375 - val_loss: 0.6913\n",
      "Epoch 2/20\n",
      "\u001b[1m 1/23\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.3750 - loss: 0.7063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.3750 - loss: 0.7063 - val_accuracy: 0.5500 - val_loss: 0.6896\n",
      "Epoch 3/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5621 - loss: 0.6911 - val_accuracy: 0.5500 - val_loss: 0.7052\n",
      "Epoch 4/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.7678 - val_accuracy: 0.4563 - val_loss: 0.6996\n",
      "Epoch 5/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5226 - loss: 0.6978 - val_accuracy: 0.5375 - val_loss: 0.6928\n",
      "Epoch 6/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.6883 - val_accuracy: 0.5625 - val_loss: 0.6859\n",
      "Epoch 7/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.5508 - loss: 0.6958 - val_accuracy: 0.5500 - val_loss: 0.6904\n",
      "Epoch 8/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5437 - val_loss: 0.6911\n",
      "Epoch 9/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.5395 - loss: 0.6933 - val_accuracy: 0.5562 - val_loss: 0.6902\n",
      "Epoch 10/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6875 - loss: 0.6754 - val_accuracy: 0.5437 - val_loss: 0.6896\n",
      "Epoch 11/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.5395 - loss: 0.6867 - val_accuracy: 0.5625 - val_loss: 0.7454\n",
      "Epoch 12/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.8197 - val_accuracy: 0.5625 - val_loss: 0.6854\n",
      "Epoch 13/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.5508 - loss: 0.6927 - val_accuracy: 0.5688 - val_loss: 0.6868\n",
      "Epoch 14/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7500 - loss: 0.6668 - val_accuracy: 0.5437 - val_loss: 0.6896\n",
      "Epoch 15/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.5565 - loss: 0.6908 - val_accuracy: 0.5500 - val_loss: 0.6894\n",
      "Epoch 16/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7472 - val_accuracy: 0.5625 - val_loss: 0.6858\n",
      "Epoch 17/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5650 - loss: 0.6906 - val_accuracy: 0.5500 - val_loss: 0.6885\n",
      "Epoch 18/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.4375 - loss: 0.7032 - val_accuracy: 0.5500 - val_loss: 0.6890\n",
      "Epoch 19/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.5537 - loss: 0.6921 - val_accuracy: 0.5375 - val_loss: 0.6905\n",
      "Epoch 20/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6875 - loss: 0.6637 - val_accuracy: 0.5688 - val_loss: 0.6846\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_generator.samples // batch_size\n",
    "validation_steps = max(1, test_generator.samples // batch_size)  \n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68abe864",
   "metadata": {},
   "source": [
    "наша модель працює на угад +- що не дуже добре"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813ca0c",
   "metadata": {},
   "source": [
    "збереження і оцінка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db064b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.5600 - loss: 0.6868\n",
      "\n",
      "compile_metrics: 56.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save_weights('teeth_model.weights.h5')\n",
    "\n",
    "\n",
    "scores = model.evaluate(test_generator)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3aa94c",
   "metadata": {},
   "source": [
    "Дає точість +- 50/50, що не дуже добре, так як наша модель може просто завчити, спробуємо надати менше поколінь\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f8201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "\n",
    "train_data_dir = 'train'  \n",
    "test_data_dir = 'test'   \n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Вхідна форма для моделі\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "627dcb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370 images belonging to 2 classes.\n",
      "Found 174 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea3000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.5593 - loss: 0.6911 - val_accuracy: 0.5500 - val_loss: 0.6896\n",
      "Epoch 2/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6250 - loss: 0.6804 - val_accuracy: 0.5312 - val_loss: 0.6912\n",
      "Epoch 3/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.5537 - loss: 0.6896 - val_accuracy: 0.5250 - val_loss: 0.6925\n",
      "Epoch 4/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6875 - loss: 0.6643 - val_accuracy: 0.5562 - val_loss: 0.6870\n",
      "Epoch 5/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.5537 - loss: 0.6916 - val_accuracy: 0.5500 - val_loss: 0.6890\n",
      "Epoch 6/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6875 - loss: 0.6719 - val_accuracy: 0.5562 - val_loss: 0.6876\n",
      "Epoch 7/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5650 - loss: 0.6887 - val_accuracy: 0.5500 - val_loss: 0.6884\n",
      "Epoch 8/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.4375 - loss: 0.7014 - val_accuracy: 0.5312 - val_loss: 0.6912\n",
      "Epoch 9/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.5565 - loss: 0.6906 - val_accuracy: 0.5625 - val_loss: 0.6872\n",
      "Epoch 10/10\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.6250 - loss: 0.6764 - val_accuracy: 0.5813 - val_loss: 0.6840\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_generator.samples // batch_size\n",
    "validation_steps = max(1, test_generator.samples // batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0156806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.5517 - loss: 0.6883\n",
      "\n",
      "compile_metrics: 55.17%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save_weights('teeth_model_2.weights.h5')\n",
    "\n",
    "\n",
    "scores = model.evaluate(test_generator)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0650f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370 images belonging to 2 classes.\n",
      "Found 174 images belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.5593 - loss: 0.6908 - val_accuracy: 0.5688 - val_loss: 0.6851\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6250 - loss: 0.6581 - val_accuracy: 0.5562 - val_loss: 0.6939\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.5537 - loss: 0.6888 - val_accuracy: 0.5625 - val_loss: 0.6855\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6875 - loss: 0.6478 - val_accuracy: 0.5375 - val_loss: 0.6936\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.5734 - loss: 0.6849 - val_accuracy: 0.5500 - val_loss: 0.6888\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 0.7670 - val_accuracy: 0.5562 - val_loss: 0.6865\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.5565 - loss: 0.6878 - val_accuracy: 0.5375 - val_loss: 0.6897\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6250 - loss: 0.6789 - val_accuracy: 0.5562 - val_loss: 0.6865\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.5593 - loss: 0.6883 - val_accuracy: 0.5562 - val_loss: 0.6873\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5625 - loss: 0.6874 - val_accuracy: 0.5312 - val_loss: 0.6904\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.5650 - loss: 0.6854 - val_accuracy: 0.5625 - val_loss: 0.6853\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.4375 - loss: 0.7146 - val_accuracy: 0.5688 - val_loss: 0.6844\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.5537 - loss: 0.6885 - val_accuracy: 0.5562 - val_loss: 0.6857\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6250 - loss: 0.6796 - val_accuracy: 0.5625 - val_loss: 0.6849\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.5565 - loss: 0.6886 - val_accuracy: 0.5625 - val_loss: 0.6851\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6250 - loss: 0.6775 - val_accuracy: 0.5625 - val_loss: 0.6850\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.5565 - loss: 0.6887 - val_accuracy: 0.5500 - val_loss: 0.6872\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6250 - loss: 0.6776 - val_accuracy: 0.5625 - val_loss: 0.6857\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.5593 - loss: 0.6875 - val_accuracy: 0.5562 - val_loss: 0.6848\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6844 - val_accuracy: 0.5625 - val_loss: 0.6835\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 141ms/step - accuracy: 0.5508 - loss: 0.6872 - val_accuracy: 0.5375 - val_loss: 0.6892\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6250 - loss: 0.6795 - val_accuracy: 0.5688 - val_loss: 0.6827\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.5706 - loss: 0.6848 - val_accuracy: 0.5750 - val_loss: 0.6795\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.7778 - val_accuracy: 0.5437 - val_loss: 0.6877\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.5593 - loss: 0.6853 - val_accuracy: 0.5562 - val_loss: 0.6862\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.6775 - val_accuracy: 0.5625 - val_loss: 0.6834\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.5650 - loss: 0.6853 - val_accuracy: 0.5500 - val_loss: 0.6848\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.4375 - loss: 0.6964 - val_accuracy: 0.5437 - val_loss: 0.6866\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.5678 - loss: 0.6834 - val_accuracy: 0.5500 - val_loss: 0.6865\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 0.7279 - val_accuracy: 0.5375 - val_loss: 0.6870\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5650 - loss: 0.6841 - val_accuracy: 0.5437 - val_loss: 0.6965\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.4375 - loss: 0.7251 - val_accuracy: 0.5437 - val_loss: 0.6869\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.5565 - loss: 0.6869 - val_accuracy: 0.5312 - val_loss: 0.6899\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.6250 - loss: 0.6756 - val_accuracy: 0.5688 - val_loss: 0.6821\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5565 - loss: 0.6899 - val_accuracy: 0.5312 - val_loss: 0.6901\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6250 - loss: 0.6673 - val_accuracy: 0.5500 - val_loss: 0.6855\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5593 - loss: 0.6843 - val_accuracy: 0.5625 - val_loss: 0.6822\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.6856 - val_accuracy: 0.5500 - val_loss: 0.6844\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5598 - loss: 0.6859 - val_accuracy: 0.5750 - val_loss: 0.6797\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7024 - val_accuracy: 0.5625 - val_loss: 0.6822\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5508 - loss: 0.6850 - val_accuracy: 0.5625 - val_loss: 0.6799\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7500 - loss: 0.6560 - val_accuracy: 0.5562 - val_loss: 0.6818\n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.5565 - loss: 0.6835 - val_accuracy: 0.5500 - val_loss: 0.6817\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6250 - loss: 0.6761 - val_accuracy: 0.5562 - val_loss: 0.6825\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.5621 - loss: 0.6817 - val_accuracy: 0.5375 - val_loss: 0.6831\n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7219 - val_accuracy: 0.5625 - val_loss: 0.6801\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5508 - loss: 0.6862 - val_accuracy: 0.5625 - val_loss: 0.6819\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.6777 - val_accuracy: 0.5625 - val_loss: 0.6813\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5650 - loss: 0.6844 - val_accuracy: 0.5625 - val_loss: 0.6812\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4375 - loss: 0.6830 - val_accuracy: 0.5500 - val_loss: 0.6838\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.5537 - loss: 0.6919 - val_accuracy: 0.5625 - val_loss: 0.6818\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.6875 - loss: 0.6679 - val_accuracy: 0.5688 - val_loss: 0.6815\n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.5678 - loss: 0.6825 - val_accuracy: 0.5437 - val_loss: 0.6818\n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 0.7212 - val_accuracy: 0.5562 - val_loss: 0.6786\n",
      "Epoch 55/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5508 - loss: 0.6882 - val_accuracy: 0.5500 - val_loss: 0.6822\n",
      "Epoch 56/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6250 - loss: 0.6494 - val_accuracy: 0.5375 - val_loss: 0.6853\n",
      "Epoch 57/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.5650 - loss: 0.6836 - val_accuracy: 0.5625 - val_loss: 0.6771\n",
      "Epoch 58/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4375 - loss: 0.6879 - val_accuracy: 0.5500 - val_loss: 0.6798\n",
      "Epoch 59/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.5593 - loss: 0.6861 - val_accuracy: 0.5500 - val_loss: 0.6802\n",
      "Epoch 60/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.7245 - val_accuracy: 0.5562 - val_loss: 0.6759\n",
      "Epoch 61/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.5650 - loss: 0.6830 - val_accuracy: 0.5437 - val_loss: 0.6835\n",
      "Epoch 62/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4375 - loss: 0.7203 - val_accuracy: 0.5500 - val_loss: 0.6819\n",
      "Epoch 63/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5650 - loss: 0.6792 - val_accuracy: 0.5562 - val_loss: 0.6786\n",
      "Epoch 64/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.7499 - val_accuracy: 0.5500 - val_loss: 0.6820\n",
      "Epoch 65/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.5480 - loss: 0.6864 - val_accuracy: 0.5500 - val_loss: 0.6784\n",
      "Epoch 66/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7500 - loss: 0.6708 - val_accuracy: 0.5625 - val_loss: 0.6745\n",
      "Epoch 67/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5621 - loss: 0.6829 - val_accuracy: 0.5500 - val_loss: 0.6817\n",
      "Epoch 68/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6942 - val_accuracy: 0.5437 - val_loss: 0.6829\n",
      "Epoch 69/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.5734 - loss: 0.6794 - val_accuracy: 0.5500 - val_loss: 0.6824\n",
      "Epoch 70/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3125 - loss: 0.7371 - val_accuracy: 0.5688 - val_loss: 0.6780\n",
      "Epoch 71/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5678 - loss: 0.6782 - val_accuracy: 0.5562 - val_loss: 0.6849\n",
      "Epoch 72/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3125 - loss: 0.7823 - val_accuracy: 0.5562 - val_loss: 0.6777\n",
      "Epoch 73/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.5480 - loss: 0.6821 - val_accuracy: 0.5500 - val_loss: 0.6806\n",
      "Epoch 74/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6250 - loss: 0.6664 - val_accuracy: 0.5375 - val_loss: 0.6820\n",
      "Epoch 75/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.5480 - loss: 0.6872 - val_accuracy: 0.5562 - val_loss: 0.6790\n",
      "Epoch 76/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6250 - loss: 0.6594 - val_accuracy: 0.5312 - val_loss: 0.6814\n",
      "Epoch 77/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.5593 - loss: 0.6818 - val_accuracy: 0.5312 - val_loss: 0.6824\n",
      "Epoch 78/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6875 - loss: 0.6648 - val_accuracy: 0.5375 - val_loss: 0.6795\n",
      "Epoch 79/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5452 - loss: 0.6868 - val_accuracy: 0.5437 - val_loss: 0.6828\n",
      "Epoch 80/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7500 - loss: 0.6513 - val_accuracy: 0.5437 - val_loss: 0.6798\n",
      "Epoch 81/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.5565 - loss: 0.6880 - val_accuracy: 0.5375 - val_loss: 0.6886\n",
      "Epoch 82/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.6829 - val_accuracy: 0.5250 - val_loss: 0.6899\n",
      "Epoch 83/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5565 - loss: 0.6879 - val_accuracy: 0.5625 - val_loss: 0.6747\n",
      "Epoch 84/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6250 - loss: 0.6799 - val_accuracy: 0.5625 - val_loss: 0.6759\n",
      "Epoch 85/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5537 - loss: 0.6817 - val_accuracy: 0.5375 - val_loss: 0.6818\n",
      "Epoch 86/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6875 - loss: 0.6829 - val_accuracy: 0.5562 - val_loss: 0.6783\n",
      "Epoch 87/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5537 - loss: 0.6875 - val_accuracy: 0.5562 - val_loss: 0.6765\n",
      "Epoch 88/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6250 - loss: 0.6437 - val_accuracy: 0.5500 - val_loss: 0.6770\n",
      "Epoch 89/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5621 - loss: 0.6826 - val_accuracy: 0.5500 - val_loss: 0.6806\n",
      "Epoch 90/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5625 - loss: 0.6667 - val_accuracy: 0.5312 - val_loss: 0.6816\n",
      "Epoch 91/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.5706 - loss: 0.6774 - val_accuracy: 0.5312 - val_loss: 0.6829\n",
      "Epoch 92/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7036 - val_accuracy: 0.5375 - val_loss: 0.6794\n",
      "Epoch 93/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.5516 - loss: 0.6833 - val_accuracy: 0.5750 - val_loss: 0.6708\n",
      "Epoch 94/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6948 - val_accuracy: 0.5625 - val_loss: 0.6792\n",
      "Epoch 95/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.5593 - loss: 0.6847 - val_accuracy: 0.5437 - val_loss: 0.6801\n",
      "Epoch 96/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6937 - val_accuracy: 0.5562 - val_loss: 0.6778\n",
      "Epoch 97/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5537 - loss: 0.6818 - val_accuracy: 0.5437 - val_loss: 0.6793\n",
      "Epoch 98/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7049 - val_accuracy: 0.5437 - val_loss: 0.6776\n",
      "Epoch 99/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.5508 - loss: 0.6768 - val_accuracy: 0.5625 - val_loss: 0.6758\n",
      "Epoch 100/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.4375 - loss: 0.7336 - val_accuracy: 0.5437 - val_loss: 0.6797\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.5517 - loss: 0.6780\n",
      "\n",
      "compile_metrics: 55.17%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "\n",
    "train_data_dir = 'train'  \n",
    "test_data_dir = 'test'   \n",
    "\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "# Вхідна форма для моделі\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "steps_per_epoch = train_generator.samples // batch_size\n",
    "validation_steps = max(1, test_generator.samples // batch_size)  \n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps)\n",
    "\n",
    "\n",
    "model.save_weights('teeth_model_3.weights.h5')\n",
    "\n",
    "\n",
    "scores = model.evaluate(test_generator)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324ebea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d49ad4d",
   "metadata": {},
   "source": [
    "бачимо що модель все одно дуже погано працює\n",
    "\n",
    "Одна з причин це те що у нас пащі з зубами які у різному стані. Тому варто розбивати на категорії і ідентифікувати хвороби вже по категоріях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c5465f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тренувальні дані збережено\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "base_dir = 'data_clas'\n",
    "\n",
    "\n",
    "train_dir = 'train_clas'\n",
    "test_dir = 'test_clas'\n",
    "\n",
    "\n",
    "test_ratio = 0.75\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for class_name in os.listdir(base_dir):\n",
    "    class_path = os.path.join(base_dir, class_name)\n",
    "\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "    random.shuffle(files)\n",
    "\n",
    "    test_size = int(len(files) * test_ratio)\n",
    "    test_files = files[:test_size]\n",
    "    train_files = files[test_size:]\n",
    "\n",
    "    for f in train_files:\n",
    "        src = os.path.join(class_path, f)\n",
    "        dst = os.path.join(train_dir, class_name, f)\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "    for f in test_files:\n",
    "        src = os.path.join(class_path, f)\n",
    "        dst = os.path.join(test_dir, class_name, f)\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "\n",
    "print(f\"Тренувальні дані збережено\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a22cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train_clas'\n",
    "test_dir = 'test_clas'\n",
    "\n",
    "\n",
    "img_height, img_width = 150, 150\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63752fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 106 images belonging to 6 classes.\n",
      "Found 24 images belonging to 6 classes.\n",
      "Found 387 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  \n",
    ")\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88acc46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98e77b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c88d32aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 144ms/step - accuracy: 0.0439 - loss: 1.8315  \n",
      "\n",
      " Точність: 4.3928 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"\\n Точність: {test_acc*100:.4f} %\")\n",
    "\n",
    "model.save('teeth_classifier.keras')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "077a8b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 152ms/step\n",
      "Класи: {'BDC-BDR': 0, 'Caries': 1, 'Fractured Teeth': 2, 'Healthy Teeth': 3, 'Impacted teeth': 4, 'Infection': 5}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "print(\"Класи:\", test_generator.class_indices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
